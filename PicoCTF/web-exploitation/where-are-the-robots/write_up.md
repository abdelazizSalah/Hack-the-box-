- robots.txt is a file used by most of websites to explicitly tell crawlers which pages it should not crawl or visit. 
- So usually it exists on the same level of the website. 
- So all we need to do is to check for this endpoint : robots.txt
- In which we can see a Disallowed html page, go to it and you will find the flag: picoCTF{ca1cu1at1ng_Mach1n3s_477ce}